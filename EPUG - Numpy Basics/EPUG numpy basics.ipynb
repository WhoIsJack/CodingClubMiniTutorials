{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPUG on Numpy Basics\n",
    "\n",
    "*by Jonas Hartmann on 31.03.2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Do Numb3rs Good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we have some samples and some measurements for each sample\n",
    "\n",
    "data = [[4,5,1,6,2,4,6,7,8,3,1],   # Sample 1\n",
    "        [8,8,1,5,7,3,9,3,5,7,1],   # Sample 2 \n",
    "        [7,6,8,3,3,6,4,5,5,7,3]]   # Sample 3\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize the data by dividing each measurement by the sample's maximum\n",
    "\n",
    "data_normed = []\n",
    "for sample in data:\n",
    "    max_measure = max(sample)\n",
    "    norm_measure = []\n",
    "    for m in sample:\n",
    "        norm_measure.append(m / max_measure)\n",
    "    data_normed.append(norm_measure)\n",
    "    \n",
    "print(data_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do the same in numpy\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "data_normed = data / np.max(data, axis=1, keepdims=True)\n",
    "\n",
    "print(data_normed)\n",
    "\n",
    "# ->> Much simpler code and a substantial speed improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Notes from live session:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, numpy attempts to broadcast along the last axis\n",
    "\n",
    "# Given these shapes...\n",
    "print('1:', data.shape)\n",
    "print('2:', np.max(data, axis=1).shape)\n",
    "\n",
    "# ...broadcasting will fail:\n",
    "#data_normed = data / np.max(data, axis=1)\n",
    "\n",
    "# If the last axis was consistent...\n",
    "print('3:', data.T.shape)\n",
    "print('4:', np.max(data, axis=1).shape)\n",
    "\n",
    "# ...it would work (but the result is transposed)\n",
    "data_normed = data.T / np.max(data, axis=1)\n",
    "\n",
    "# There are several options for ensuring that broadcasting happens along the correct axis\n",
    "data_normed = data / np.max(data, axis=1, keepdims=True)  # Keeps the original dimensionality\n",
    "data_normed = data / np.max(data, axis=1).reshape((-1,1)) # Creates a view of appropriate dimensionality\n",
    "data_normed = data / np.max(data, axis=1)[:, np.newaxis]  # Adds in another dimension\n",
    "data_normed = data / np.max(data, axis=1)[:, None]        # Same as np.newaxis (shorter but less expressive)\n",
    "\n",
    "# All of these produce this shape:\n",
    "print('5:', np.max(data, axis=1)[:, None].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working With Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array properties\n",
    "\n",
    "print(data.size)\n",
    "print(data.ndim)\n",
    "print(data.shape)\n",
    "\n",
    "print(data.dtype)\n",
    "print(data_normed.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array operations applied to entire arrays\n",
    "\n",
    "print(data.sum(), np.sum(data) )  # These are equivalent\n",
    "print(data.mean(), np.mean(data) )\n",
    "print(np.percentile(data, 95) )  # Not all np functions have a method equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array operations applied to specific axes (=dimensions)\n",
    "\n",
    "print(data.sum(axis=1), np.sum(data, axis=1))\n",
    "print(data.sum(axis=0), np.sum(data, axis=0))\n",
    "\n",
    "# Note that the dimension specified is the one over which the operation\n",
    "# \"accumulates\", so it's the dimension that has \"disappeared\" in the results.\n",
    "\n",
    "# Also, note that you can accumulate over multiple dimensions using\n",
    "# tuples as the axis argument.\n",
    "d = np.random.randint(0, 10, size=(5,10,15))  # A random 3-dimensional array\n",
    "d.sum(axis=(1,2))  # Sums up both the 2nd and 3rd axis/dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations with numpy arrays apply element-wise\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(data + 100)\n",
    "print(data * 3)\n",
    "\n",
    "print(data + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays can be \"sliced\" or \"indexed into\"\n",
    "\n",
    "print(data[0])\n",
    "print(data[:,0])\n",
    "print(data[:,2:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparisons (boolean operations) apply element-wise\n",
    "\n",
    "print(data > 5)\n",
    "\n",
    "print(np.all(data > 5))\n",
    "print(np.all(data > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean numpy arrays can function as masks for indexing\n",
    "\n",
    "mask = data > 5\n",
    "print(mask.dtype)\n",
    "\n",
    "data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same with classical looping...\n",
    "\n",
    "above_5 = []\n",
    "for sample in data:\n",
    "    for m in sample:\n",
    "        if m > 5:\n",
    "            above_5.append(m)\n",
    "\n",
    "above_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy plays nice with many scientific computing tools!\n",
    "\n",
    "# Numerical tools\n",
    "from scipy.spatial import distance as dist\n",
    "dist.squareform(dist.pdist(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tools\n",
    "from scipy import stats\n",
    "stats.pearsonr(data[0], data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in data:\n",
    "    plt.plot(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Note from live session: some additional indexing tricks\n",
    "\n",
    "# Ellipsis indexing\n",
    "d = np.random.randint(0, 10, size=(5,10,15,20))  # A random 4-dimensional array\n",
    "d[:,:,:,0]   # This is the same...\n",
    "d[...,0]     # ...as this!\n",
    "d[0,:,:,0]   # This is the same...\n",
    "d[0,...,0]   # ...as this\n",
    "\n",
    "# Ellipsis indexing is useful when the number of dimensions may vary\n",
    "d[...,0]     # Indexes first element of last dimension\n",
    "data[...,0]  # Also indexes first element of last dimension (despite different ndim!)\n",
    "\n",
    "# Slice objects provide additional flexibility as they can be assigned to variables and\n",
    "# hence can be constructed and operated on outside of the actual indexing operation\n",
    "d[1:4,0,0,0]         # This is the same...\n",
    "d[slice(1,3),0,0,0]  # ...as this\n",
    "d[:,0,0,0]            # This is the same\n",
    "d[slice(None),0,0,0]  # ...as this\n",
    "#s = :           # This wouldn't work!\n",
    "s = slice(None)  # But this does!\n",
    "\n",
    "# Numpy also supplies a custom slice object that can handle multiple dimensions\n",
    "d[slice(1,5),0,0,0]  # This is the same...\n",
    "d[np.s_[1:4],0,0,0]     # ...as this\n",
    "d[:,...,0]         # This is the same...\n",
    "d[np.s_[:,...,0]]  # ...as this\n",
    "s = np.s_[:,...,0]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays Everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Microscopy images are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "img = io.imread(r'data/nuclei_DAPI_confocal.tif')\n",
    "\n",
    "print(img.shape)\n",
    "print(img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very basic thresholding\n",
    "mask = img > np.mean(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough estimate of mean nuclear intensity\n",
    "np.mean(img[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this self-explanatory online course](https://github.com/WhoIsJack/python-bioimage-analysis-tutorial) if you want to get into image analysis with python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Note from live session: always be aware of your data types!\n",
    "\n",
    "# Most microscopy images are saved in `uint8` format.\n",
    "# This is very memory efficient (only 8bit per number)\n",
    "# but it can lead to weird results if used incorrectly!\n",
    "\n",
    "# u -> unsigned, i.e. no negative numbers\n",
    "# int -> integers, i.e. whole numbers (0, 1, 2)\n",
    "# 8 -> 8bit, i.e. numbers from 0 to 255\n",
    "\n",
    "# One common source of problems is that numbers above 255 or\n",
    "# below 0 \"wrap around\". Check this out:\n",
    "img_weird = img + 100\n",
    "plt.imshow(img_weird, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time course data are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = np.load(r'data/totally_real_timecourse_data.npy')\n",
    "\n",
    "print(tc.shape)\n",
    "print(tc.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for track in tc:\n",
    "    plt.plot(track)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm maxima again, as in the initial examples\n",
    "\n",
    "tc_normed = tc / np.max(tc, axis=1, keepdims=True)\n",
    "\n",
    "for track in tc_normed:\n",
    "    plt.plot(track)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Shift tracks in time to overlay maximum slope\n",
    "\n",
    "tc_time = np.ones_like(tc) * np.arange(tc.shape[1])\n",
    "\n",
    "for t in range(tc.shape[0]):\n",
    "    smoothed = np.convolve(tc_normed[t], np.ones(5), mode='valid')  # Smoothen to make max detection robust\n",
    "    max_slope_idx = np.argmax(np.diff(smoothed)) + 2                # Detect max slope (+2 bc smoothed is shorter!)\n",
    "    tc_time[t] -= max_slope_idx                                     # Shift the time accordingly\n",
    "    \n",
    "for time, track in zip(tc_time, tc_normed):\n",
    "    plt.plot(time, track)\n",
    "plt.show()\n",
    "\n",
    "# ->> Turns out these tracks varied only in their timing and maximum value. \n",
    "#     Other than that, their dynamics are essentially identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature spaces are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a classical machine learning example dataset from scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# What do we have here \n",
    "print(iris.data.shape)     # The actual data; 150 samples by 4 features\n",
    "print(iris.feature_names)  # The 4 features that were measured (they are plant flower measures)\n",
    "print(iris.target.shape)   # The classification of the 150 samples into 3 plant species\n",
    "print(iris.target_names)   # The names of the 3 plan species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "iris_pca = PCA().fit_transform(iris.data)\n",
    "\n",
    "plt.scatter(iris_pca[:,0], iris_pca[:,1], c=iris.target)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()\n",
    "\n",
    "# ->> Looks like PC 1 distinguishes the species quite well, especially setosa\n",
    "#     on the left, whereas PC 2 captures something unrelated to species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your data are arrays!\n",
    "\n",
    "...probably. ;p\n",
    "\n",
    "There are cases where arrays are imperfect, such as when there is a different number of measurements/timepoints/whatever for each sample. In such cases, don't hesitate to combine numpy with classical python, e.g. using a list of arrays or a dict of arrays.\n",
    "\n",
    "Another common case where pure numpy arrays are suboptimal is if you have different data types (e.g. a machine learning dataset with some numerical features, some boolean features, some string features). In such cases, have a look at the pandas module and its dataframe objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To learn more...\n",
    "\n",
    "...I recommend starting some toy projects! For example, try to analyze some data you're interested in (from your science, from your holiday pictures, from something in the news, etc...) or try to come up with a simulation for something you find interesting (e.g. a simple simulation of evolution, or something based on differential equations if you're familiar with those, etc...). This is a great way of learning in my experience. \n",
    "\n",
    "Alternatively, check out the links to course materials and tutorials on the Bio-IT webpage [here](https://bio-it.embl.de/online-learning/) and [here](https://bio-it.embl.de/coding-club/curated-tutorials/) or try your hand at some coding challenges such as [Advent of Code](https://adventofcode.com/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (python3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
